{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow的使用笔记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 张量\n",
    "tf.Tensor类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 张量不可修改，只能新建\n",
    "2. 张量有统一的dtype，比如tf.float32, tf.int32, tf.string, tf.bool等。它们都是tf.dtypes.DType类(查看类方法：https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)\n",
    "3. 张量有axis，每个axis上的元素数量相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建张量的函数有：\n",
    "1. tf.constant()\n",
    "2. tf.ones()\n",
    "3. tf.zeros()\n",
    "\n",
    "创建的时候可以设定dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar. This will be an int32 tensor by default; see \"dtypes\" below.\n",
    "# scalar的shape是空的\n",
    "rank_0_tensor = tf.constant(4)\n",
    "print(rank_0_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以认为一维张量是列向量\n",
    "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
    "print(rank_1_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以在创建张量的时候设定dtype\n",
    "rank_2_tensor = tf.constant([[1, 2],\n",
    "                             [3, 4],\n",
    "                             [5, 6]], dtype=tf.float16)\n",
    "print(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3维张量\n",
    "rank_3_tensor = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],])\n",
    "# 4维张量\n",
    "rank_4_tensor = tf.zeros([3, 2, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor和array互相转化\n",
    "\n",
    "tf.Tensor -> np.array\n",
    "1. gen_array = gen_tensor.numpy()\n",
    "2. gen_array = np.array(gen_tensor)\n",
    "\n",
    "np.array -> tf.Tensor\n",
    "1. gen_tensor = tf.constant(gen_array)  \n",
    "由于gen_array是可变的数据，变动gen_array可能会导致gen_tensor自动变化（也可能没变，很奇怪）。见下面这个trunk\n",
    "2. gen_array -> list -> tf.Tensor\n",
    "可以从list构造tf.Tensor，即：  \n",
    "gen_list = gen_array.tolist()  \n",
    "gen_tensor = tf.constant(gen_list)  \n",
    "这样即使变动gen_list，gen_tensor也不会随之改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多跑几次，有时候是[1.,2.,3.]，有时又会是[7.,2.,3.]\n",
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = tf.constant(a)\n",
    "a[0] = 7.0\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引  \n",
    "标量索引 -> 移除轴  \n",
    "切片:索引 -> 保留轴  \n",
    "对于多轴tensor也是如此。虽然多轴tensor可以不在每个轴上都索引，但推荐对每个轴都索引，获得想要的数据格式（标准化代码）  \n",
    "比如：  \n",
    "    rank_3_tensor[0, :, :] #取出了第一个二维tensor，移除了第一个轴，结果是二维tensor\n",
    "    rank_3_tensor[0:1, :, :] #也取出了第一个二维tensor，但保留了第一个轴，结果是三维tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在最后一维作标量索引，所以最后一维被移除了。原倒数第二维度成了最后一维，变成了横向\n",
    "print(rank_3_tensor[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在最后一维作切片索引，最后一维没有移除\n",
    "print(rank_3_tensor[:, :, -1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor的shape及其转换\n",
    "1. 用.shape属性 或 .get_shape()方法取出gen_tensor的shape。返回的是tf.TensorShape类。shape是每个轴的长度\n",
    "2. tensor的轴的排列顺序（从左到右）一般从全局到局部；如果二维表示地话，从左到右一般是大的纵向，到小的纵向，到最小的纵向，只有最后一维是横向\n",
    "    1. 最后一个轴一般是features，所以shape[-1]一般是每个样本元素的features个数（样本元素组合成为单个样本）\n",
    "    2. 从倒数第二个轴开始，到正数第二个轴，一般是空间位置信息\n",
    "    3. 第一个轴一般是批次index\n",
    "\n",
    "很多原始数据的轴排列方式并不一定符合上述论述，可能需要对应调整  \n",
    "TensorFlow 采用 C 样式的“行优先”内存访问顺序，即最右侧的索引值递增对应于内存中的单步位移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape  \n",
    "  \n",
    "TensorFlow 采用 C 样式的“行优先”内存访问顺序，即\n",
    "1. 最右侧的轴的索引值递增对应于内存中的单步位移。设最右侧的维度为n_right_1st\n",
    "2. 第二右侧的轴的索引值递增对应内存中的n_right_1st步位移  \n",
    "3. 以此类推，一直到第一个轴\n",
    "\n",
    "所以如果数据：\n",
    "1. 生产数据为(batch_size, space_info, features_dims)的结构。  \n",
    "这里features_dims是一个样本元素的特征维度。对一个样本来说，其特征维度是所有样本元素的特征维度之和。  \n",
    "2. reshape到(batch_size, -1)的二维结构。  \n",
    "此reshape过程把batch维度之外的轴，全部移除。这样一来，每一个样本（共有batch_size个样本），其所有特征在内存中是连续放置的。  \n",
    "\n",
    "总结：要定义清楚样本是什么。reshape的结果是：一个样本一行，其所有特征随着列横向排列，它们在内存中连续\n",
    "注意：reshape只是重新组织指向内存地址的方式，数据在内存中的放置并不会改边。所以reshape很快。但reshape在返回赋值的时候是复制数据到新的内存的。\n",
    "reshape无法换轴（横纵转置意味着数据在内存中的排列方式改变）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reshape(rank_3_tensor, [3*2, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transpose  \n",
    "  \n",
    "要改变数据在内存中的具体放置，需要用tf.transpose。tf.transpose返回内存连续的、转置之后的数组  \n",
    "tf.transpose作用在二维数组上（设数组宽度为w，高度为h），就是横纵对换  \n",
    "--> 从左上角(0,0)开始，往右单步位移，往下是w步位移，此为原数组\n",
    "--> 往右是w步位移，往下是单步位移，此为转置后数组\n",
    "\n",
    "如何理解三个及以上轴的transpose, 设tensor的形状为（b，h，w）\n",
    "--> 从（0，0，0）开始，第三轴往右是单步位移，第二轴往下是w步位移，第一轴往下是h*w步位移，此为原数组\n",
    "--> 从（0，0，0）开始，第三轴往右是单步位移，第二轴往下是h*w步位移，第一轴往下是w步位移，此为(1,0,2)转置数组\n",
    "--> 从（0，0，0）开始，第三轴往右是h*w步位移，第二轴往下是w步位移，第一轴往下是单步位移，此为(2,1,0)转置数组\n",
    "以此类推，其实就是步长的对换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.transpose(rank_3_tensor, perm=(1,0,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor的dtype及其转换\n",
    "1. 用.dtype属性取出gen_tensor的dtype。返回的是tf.DType类\n",
    "\n",
    "cast  \n",
    "  \n",
    "可以改变tensor的数据类型，使用tf.cast。需制定转变后的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_3_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_3_tensor_fl = tf.cast(rank_3_tensor, dtype=tf.float16)\n",
    "rank_3_tensor_fl.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor运算及broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1,2,3])\n",
    "x = tf.reshape(x, [3,1])\n",
    "y = tf.range(1, 5)\n",
    "print(x, \"\\n\")\n",
    "print(y, \"\\n\")\n",
    "print(tf.multiply(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tf.broadcast_to()展示tensor按照shape广播的结果\n",
    "print(tf.broadcast_to(x, [3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转化为tensor\n",
    "tf.convert_to_tensor()方法可以np.array, TensorShape, list和tf.Variable转化为tf.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不规则张量 ragged tensor\n",
    "不规则张量是tf.RaggedTensor类  \n",
    "创建： tf.ragged.constant()  \n",
    "shape：.shape属性返回shape，None说明该轴长度未知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragged_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9]]\n",
    "ragged_tensor = tf.ragged.constant(ragged_list)\n",
    "print(ragged_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragged_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 字符串张量\n",
    "tf.string是一种原子类型的dtype，类似tf.int32或tf.float64，无法像python字符串一样索引。tf.string的长度并不是张量的一个轴。\n",
    "创建：tf.constant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标量scalra字符串张量\n",
    "# 前缀b代表字节字符串\n",
    "scalar_string_tensor = tf.constant(\"Gray wolf\")\n",
    "print(scalar_string_tensor)\n",
    "# 使用unicode字符串，加前缀u\n",
    "# unicode_string_tensor = tf.constant(u\"Gray wolf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串向量\n",
    "tensor_of_strings = tf.constant([\"Gray wolf\", \"Quick brown fox\", \"Lazy dog\"])\n",
    "print(tensor_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字符串操作  \n",
    "由于tf.string是原子类型的dtype，要使用tf.strings中的方法来对tensor of string作操作，比如：\n",
    "1. 分割：tf.strings.split() -> 从tensor of string中分割每个字符串元素到token。如果分割的长度不一，则返回tf.RaggedTensor\n",
    "2. 转换数字：tf.strings.to_number() -> 从tensor of string中转化数字字符串到数字\n",
    "3. 转换ASC码或unicode码：\n",
    "    1. tf.strings.bytes_split() -> 从tensor of string中分割每个字符串元素到字节。如果分割的长度不一，则返回tf.RaggedTensor\n",
    "    2. tf.io.decode_raw() # 详见tf.io模块\n",
    "    3. tf.strings.unicode_split() -> 从tensor of string中分割每个unicode字符串元素到字节。如果分割的长度不一，则返回tf.RaggedTensor\n",
    "    4. tf.strings.unicode_decode() -> 从tensor of string中分割每个unicode字符串元素到unicode码。如果分割的长度不一，则返回tf.RaggedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_strings = tf.strings.bytes_split(tensor_of_strings)\n",
    "print(byte_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_decode(tensor_of_strings, 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 稀疏张量 sparse tensor\n",
    "sparse tensor只储存非0元素的位置indices，非0元素的值values(与indices长度相同，且顺序一致)，稀疏张量完整表示时的dense_shape  \n",
    "以上三个参数indices、values、dense_shape都以tf.Tensor类传入tf.sparse.SparseTensor()函数，即构造了相应的sparse tensor  \n",
    "tf.sparse.to_dense(gen_sparse_tensor)即可得到稀疏特征对应的稠密特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                                       values=[1, 2],\n",
    "                                       dense_shape=[3, 4])\n",
    "print(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.sparse.to_dense(sparse_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.变量\n",
    "tf.Variable类\n",
    "\n",
    "1. 创建：tf.Variable变量需要提供一个张量tf.Tensor作为初始值。此初始值的dtype和shape确定了tf.Variable的dtype和shape\n",
    "2. 运算：大部分tf.Tensor的运算也可以用在tf.Variable上。比如tf.argmax。具体运算的时候，是使用变量的支持张量执行运算\n",
    "3. 转换：tf.Tensor、ndarray等(tf.convert_to_tensor(var), var.numpy())\n",
    "4. 赋值：gen_variable.assign()重新分配张量，有如下特点：\n",
    "    1. 调用 assign（通常）不会分配新张量，而会重用现有张量的内存。\n",
    "    2. assign的时候不能改变shape\n",
    "    3. 还有gen_variable.assign_add(), gen_variable.assign_sub()等方法\n",
    "4. 复制、reshape、命名\n",
    "    1. 复制：从变量a创建变量b b = tf.Variable(a) 会复制支持张量，两个变量不共享内存空间\n",
    "    2. 重构：reshape是先复制再reshape，并不会reshape原本的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)> \n",
      "\n",
      "<dtype: 'int32'>\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# 创建变量\n",
    "sup_tensor = tf.constant([[1,2],[3,4]])\n",
    "var = tf.Variable(sup_tensor)\n",
    "print(var, '\\n')\n",
    "print(var.dtype)\n",
    "print(var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为tensor或ndarray\n",
    "print(\"as tensor: \\n\", tf.convert_to_tensor(var))\n",
    "print('as ndarray:\\n', var.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 命名和梯度\n",
    "1. 可以在创建variable的时候，为该变量提供自定义变量名，只需设name=\"set_name\"。变量名可以重复。在模型中，变量名会自动获得且一般不重复\n",
    "2. 可以在创建variable的时候，关闭梯度，只需设trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([1,2,3], name='a')\n",
    "print(a.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自动微分和梯度\n",
    "tensorflow提供了一个tf.GradientTape()上下文管理器，来打开、关闭记录相关运算的tape。  \n",
    "在BP过程中，此tape可以拿来计算涉及的node的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. source node创建的时候发生了什么：\n",
    "    1. 由于tf.Variable()中trainable参数为True，x将是个可训练变量\n",
    "\n",
    "2. 打开tape的时候发生了什么：\n",
    "    tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "    使用watch_accessed_variables参数，以及tf.watch\n",
    "    1. 自动监控所有可训练变量：GradientTape 默认（watch_accessed_variables=True）  \n",
    "    将所有可训练变量（created by tf.Variable, where trainable=True）视为需要监控的 source node  \n",
    "    2. 对于不可训练的张量（比如tf.constant），可以使用tape.watch()对其进行“监控”  \n",
    "    还可以设定watch_accessed_variables=False，然后使用tf.watch()精确控制需要监控哪些变量  \n",
    "    3. 可选persistent=True。默认persistent=False\n",
    "\n",
    "3. 前向计算过程中发生了什么：\n",
    "    1. 依赖source node的计算节点被创建。直到target node。target node一般是标量。\n",
    "    2. 连接source node、中间节点、target的operation，以及assign、tape.stop_recording()截断，得到计算图  \n",
    "    3. tape.reset()清除已有的计算图\n",
    "\n",
    "4. 关闭tape的时候发生了什么：\n",
    "    1. 如果persistent=False，那么默认tape在进行一次求导之后就销毁计算图，从而节约内存\n",
    "    2. 如果设定persistent=True，那么可以在同一个计算图下多次求导。在求导过程全部结束后要del tape来释放资源\n",
    "\n",
    "如果这里调用tape.watched_variables()，可以看到所有被tape监控的source node\n",
    "\n",
    "5. 反向计算过程中发生了什么：\n",
    "    1. tape.gradient(target, sources, unconnected_gradients, output_gradients)求target对sources的偏导数\n",
    "    其中：\n",
    "        1. target一般是标量tensor，且一般就是loss\n",
    "        2. sources可以传入列表或字典的嵌套组合。gradient返回的grad和sources具有相同的结构。\n",
    "        3. unconnected_gradients可选'none'和'zero'，是指source node和target node在计算图上不联通时，返回的导数\n",
    "        4. output_gradients默认为None，不操作；如果要传入的话，它是权重。gradient乘以权重之后再输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向计算-tape记录\n",
    "x = tf.Variable(3.0) # source node创建\n",
    "with tf.GradientTape() as tape: # 打开tape\n",
    "    y = x**2 # 依赖source node的节点创建。并且这里y是target node。y作为运算结果，类型一般是tensor\n",
    "    # x依据tensor的计算而来，所以y是tensor\n",
    "# 关闭tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向计算-tape求解grad\n",
    "dy_dx = tape.gradient(y, x) # 传入target node，传入source node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target node一般是标量\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient的shape与source node相同\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source node\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w') # w是3x2的矩阵变量\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b') # b是长度为2的变量，\n",
    "x = [[1., 2., 3.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.99978405,  6.6519294 ]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x @ w + b # tensor是横向量，或者更准确如下：\n",
    "# y = x @ w + tf.reshape(b, (1,2))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b # 中间节点\n",
    "    loss = tf.reduce_mean(y**2) # target node，也就是loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求梯度\n",
    "# 传入列表结构\n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "# dl_dw: loss相对w矩阵的偏导数\n",
    "# dl_db: loss相对b向量的偏导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.99978405,  6.6519294 ],\n",
       "       [-1.9995681 , 13.303859  ],\n",
       "       [-2.9993522 , 19.955788  ]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求梯度\n",
    "# 传入字典结构\n",
    "my_vars = {\n",
    "    'w':w,\n",
    "    'b':b\n",
    "}\n",
    "grad = tape.gradient(loss, my_vars) # 返回的grad也是一个字典结构\n",
    "grad['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "358c856c2c9243453432cede0b7a0ffead5a19b5df3cbac804ae558d1878f503"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
