{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建tf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、所有的model(layer)都继承tf.Module类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 00:21:45.111252: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-20 00:21:45.125611: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbc771973c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-20 00:21:45.125624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eager execute\n",
    "# 动态图\n",
    "class SimpleModule(tf.Module):\n",
    "    def __init__(self, name=None): # 模型初始化，定义所需变量和常量\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.0, name='train_me')\n",
    "        self.non_trainable_varibale = tf.Variable(5.0, trainable=False, name=\"do_not_train_me\")\n",
    "    def __call__(self, x): # 前向过程\n",
    "        return self.a_variable * x + self.non_trainable_varibale\n",
    "\n",
    "simple_module = SimpleModule(name=\"simple\")\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何获得静态图？  \n",
    "在tfnotes_graph中学到，用tf.function修饰函数usr_def_func，修饰器会捕捉函数usr_def_func中的所有tf.Op和python logic，构成计算图（概念中的）。第一次调用该函数的时候，会执行trace，在内存中真正创建tf.Graph（tf.Autograph）  \n",
    "usr_def_func.get_concrete_function(input_data).graph.as_graph_def()，可以打印计算图  \n",
    "  \n",
    "在模型的class代码中，用tf.function修饰前向过程（即__call__函数）。第一次开始前向过程的时候，会执行trace，构建tf.Graph  \n",
    "可以使用  tfmodel.\\_\\_call\\_\\_.get_concrete_function(input_data).graph.as_graph_def()  的方法，打印计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph execute\n",
    "# 静态图\n",
    "class SimpleModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.0, name='train_me')\n",
    "        self.non_trainable_varibale = tf.Variable(5.0, trainable=False, name=\"do_not_train_me\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x): # tf.function修饰前向过程，得到静态图的模型\n",
    "        return self.a_variable * x + self.non_trainable_varibale\n",
    "\n",
    "simple_module = SimpleModule(name='simplegraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"x\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"_user_specified_name\"\n",
       "    value {\n",
       "      s: \"x\"\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul\"\n",
       "  op: \"Mul\"\n",
       "  input: \"ReadVariableOp\"\n",
       "  input: \"x\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add/ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add/ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"add/ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"mul\"\n",
       "  input: \"add/ReadVariableOp\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"add\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 440\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印图结构\n",
    "simple_module.__call__.get_concrete_function(tf.constant(5.0)).graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、用model构建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module): # dense layer\n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros(out_features), name='b')\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.dense_1 = Dense(in_features=3, out_features=3)\n",
    "        self.dense_2 = Dense(in_features=3, out_features=2)\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x) # 重引用不会创建node\n",
    "        return self.dense_2(x)\n",
    "\n",
    "my_model = SequentialModule(name=\"mlp_2layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"x\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"_user_specified_name\"\n",
       "    value {\n",
       "      s: \"x\"\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "        dim {\n",
       "          size: 1\n",
       "        }\n",
       "        dim {\n",
       "          size: 3\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul/ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul/ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"MatMul/ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul\"\n",
       "  op: \"MatMul\"\n",
       "  input: \"x\"\n",
       "  input: \"MatMul/ReadVariableOp\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"transpose_a\"\n",
       "    value {\n",
       "      b: false\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"transpose_b\"\n",
       "    value {\n",
       "      b: false\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add/ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add/ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"add/ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"MatMul\"\n",
       "  input: \"add/ReadVariableOp\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Relu\"\n",
       "  op: \"Relu\"\n",
       "  input: \"add\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul_1/ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul_1/ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"MatMul_1/ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"MatMul_1\"\n",
       "  op: \"MatMul\"\n",
       "  input: \"Relu\"\n",
       "  input: \"MatMul_1/ReadVariableOp\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"transpose_a\"\n",
       "    value {\n",
       "      b: false\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"transpose_b\"\n",
       "    value {\n",
       "      b: false\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add_1/ReadVariableOp/resource\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_RESOURCE\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add_1/ReadVariableOp\"\n",
       "  op: \"ReadVariableOp\"\n",
       "  input: \"add_1/ReadVariableOp/resource\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add_1\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"MatMul_1\"\n",
       "  input: \"add_1/ReadVariableOp\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Relu_1\"\n",
       "  op: \"Relu\"\n",
       "  input: \"add_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"Relu_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_FLOAT\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 440\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.__call__.get_concrete_function(tf.constant([[1., 2., 3.]])).graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Module类有类方法，可以展示它自动收集的tf.Variable实例和tf.Module实例。初始化一个实例之后就可以看到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 00:22:02.143245: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.Dense at 0x7fbc837425e0>, <__main__.Dense at 0x7fbc83628fa0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 1.2392718 ,  1.1902896 , -1.2150244 ],\n",
      "       [-2.1295872 , -0.5849101 ,  1.2354501 ],\n",
      "       [ 1.953534  ,  0.79020137,  2.3628767 ]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.502447  ,  0.67322606],\n",
      "       [ 1.5595437 , -0.8026046 ],\n",
      "       [-1.2609985 , -0.96091294]], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in my_model.variables:\n",
    "    print(var, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all trainable variables\n",
    "simple_module.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,\n",
       " <tf.Variable 'do_not_train_me:0' shape=() dtype=float32, numpy=5.0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all variables\n",
    "simple_module.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、延迟构建（deferr）  \n",
    "即不是在初始化实例的时候构建内部参数变量，而是在第一次前向过程的时候，构建内部变量  \n",
    "好处是等拿到input之后，才开始构建内部参数变量。所以内部参数变量可以根据第一次input来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[4.1062264, 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FlexibleDense(tf.Module):\n",
    "    def __init__(self, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.is_built = False # 内部变量，判断计算图是否创建。第一次前向过程中会创建计算图，在此时才初始化内部变量\n",
    "        self.out_features = out_features # 传递实例初始化的参数到前向过程函数\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # create variables on first call\n",
    "        if not self.is_built: # 假如尚未built，即第一次前向过程之前\n",
    "            self.w = tf.Variable(tf.random.normal([x.shape[-1], self.out_features]), name='w') # 根据input data维度，确定in_features\n",
    "            self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "            self.is_built = True # 第一次前向过程中，内部变量被创建，且计算图被创建。设定状态变量is_built为True，避免下一次前向过程再度创建内部变量\n",
    "        \n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)\n",
    "\n",
    "class MySeqModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "        self.dense_1 = FlexibleDense(out_features=3)\n",
    "        self.dense_2 = FlexibleDense(out_features=2)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "\n",
    "my_model = MySeqModule(name='flex_mlp_2l')\n",
    "my_model(tf.constant([[1., 2., 3., 4.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存tf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、保存参数  \n",
    "tfmodel保存的参数又叫做checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../model/my_checkpoint'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkp_path = \"../../model/my_checkpoint\"\n",
    "checkpoint = tf.train.Checkpoint(model=my_model)\n",
    "checkpoint.write(chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列出以my_checkpoint命名的文件:  \n",
    ".data-00000-of-00001文件是数据分片，储存了weights的值  \n",
    ".index文件储存了weights的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../model/my_checkpoint.data-00000-of-00001\n",
      "../../model/my_checkpoint.index\n"
     ]
    }
   ],
   "source": [
    "ls ../../model/my_checkpoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
       " ('model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE', [3]),\n",
       " ('model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE', [4, 3]),\n",
       " ('model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE', [2]),\n",
       " ('model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 2])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印checkpoint文件中存储的变量\n",
    "tf.train.list_variables(chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、读取checkpoints  \n",
    "读取checkpoints前，需要从original python code中，实例化一个新的model  \n",
    "然后使用  tf.train.Checkpoint(model=new_model).restore(chkp_path)  方法来加载checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbc83753040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = MySeqModule() #初始化一个新的model，同一个类\n",
    "new_checkpoint = tf.train.Checkpoint(model=new_model)\n",
    "new_checkpoint.restore(chkp_path) #new_model读取chkp_path储存的checkoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[4.1062264, 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试一下\n",
    "new_model(tf.constant([[1., 2., 3., 4.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、保存函数  \n",
    "tf model可以在没有python源代码的情况下使用，只需计算图被建构好。  \n",
    "所以保存模型 即 保存计算图，即 使用静态图模型  \n",
    "  \n",
    "定义模型的时候，要定义静态图的模型，即用tf.function修饰__call__()前向函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 02:43:19.494711: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-20 02:43:19.512268: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa5c95a6650 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-20 02:43:19.512282: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "class MySeqModel(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.dense_1 = Dense(in_features=3, out_features=3)\n",
    "        self.dense_2 = Dense(in_features=3, out_features=2)\n",
    "    \n",
    "    @tf.function # 定义静态图模型，在第一次前向过程的时候，会创建计算图\n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "my_model = MySeqModel(name='mlp_2l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.1390929 , 0.21892878]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(tf.constant([[2., 2., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.1390929 , 0.21892878],\n",
       "       [1.1390929 , 0.21892878]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(tf.constant([[2., 2., 2.], [2., 2., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
       "array([[[1.1390929 , 0.21892878],\n",
       "        [1.1390929 , 0.21892878]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(tf.constant([[[2., 2., 2.],\n",
    "                       [2., 2., 2.]],\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(2, 3)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(2, 2)\n",
      "\n",
      "__call__(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(1, 2, 3)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(1, 2, 2)\n",
      "\n",
      "__call__(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(1, 3)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# 打印my_model 概念计算图的signatures\n",
    "print(my_model.__call__.pretty_printed_concrete_signatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、保存模型  \n",
    "最好的办法是直接保存模型，这样就会同时保存函数（graphs）和数据（weights）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../model/mlp_2l_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 02:09:28.298319: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(my_model, '../../model/mlp_2l_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成了一个指定路径的文件夹，打印文件夹里的所有文件，可以看到三个固定命名的文件：  \n",
    "assets  \n",
    "variables  \n",
    "saved_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\n",
      "drwxr-xr-x  2 lhy  staff     64  1 20 01:30 \u001b[1m\u001b[36massets\u001b[m\u001b[m/\n",
      "-rw-r--r--  1 lhy  staff  14131  1 20 02:09 saved_model.pb\n",
      "drwxr-xr-x  4 lhy  staff    128  1 20 02:09 \u001b[1m\u001b[36mvariables\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls -l ../../model/mlp_2l_saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved_model.pb文件是类似json的语言中性的序列文件，记录了计算图graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variables文件夹记录了模型的checkpoints文件（data shard和index文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "-rw-r--r--  1 lhy  staff  408  1 20 02:09 variables.data-00000-of-00001\n",
      "-rw-r--r--  1 lhy  staff  356  1 20 02:09 variables.index\n"
     ]
    }
   ],
   "source": [
    "ls -l ../../model/mlp_2l_saved_model/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、加载模型  \n",
    "从saved_model文件中加载模型，不需要python源代码，甚至不需要python解释器  \n",
    "因为.pb文件是语言中性的定义graph文件，所以会直接根据.pb文件在内存中创建计算图  \n",
    "所以:  \n",
    "  加载的模型不是原模型class的实例instance  \n",
    "  加载的模型，只能使用already-defined签名  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load('../../model/mlp_2l_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.09257771, 0.2780159 ],\n",
       "       [0.09257771, 0.2780159 ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试一下符合的signature input\n",
    "new_model(tf.constant([[2., 2., 2.], [2., 2., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * Tensor(\"x:0\", shape=(2, 2, 3), dtype=float32)\n  Keyword arguments: {}\n\nExpected these arguments to match one of the following 3 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(1, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(2, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(1, 2, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wt/bk30kdrd0vl2vsz2xspflyg80000gp/T/ipykernel_70015/1854251448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 测试一下不符合的signature input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m new_model(tf.constant([[[2., 2., 2.],\n\u001b[0m\u001b[1;32m      3\u001b[0m                        [2., 2., 2.]],\n\u001b[1;32m      4\u001b[0m                        [[2., 2., 2.],\n\u001b[1;32m      5\u001b[0m                        [2., 2., 2.]],\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mrestored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m           \u001b[0;34m\"Option {}:\\n  {}\\n  Keyword arguments: {}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           .format(index + 1, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 251\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;34m\"Could not find matching function to call loaded from the SavedModel. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;34m\"Got:\\n  {}\\n  Keyword arguments: {}\\n\\nExpected \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find matching function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * Tensor(\"x:0\", shape=(2, 2, 3), dtype=float32)\n  Keyword arguments: {}\n\nExpected these arguments to match one of the following 3 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(1, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(2, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(1, 2, 3), dtype=tf.float32, name='x')\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "# 测试一下不符合的signature input\n",
    "new_model(tf.constant([[[2., 2., 2.],\n",
    "                       [2., 2., 2.]],\n",
    "                       [[2., 2., 2.],\n",
    "                       [2., 2., 2.]],\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图Graph可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用tf.function修饰前向过程__call__的模型，会在第一次前向过程中，在内存中生成静态图tf.Graph  \n",
    "生成静态图tf.Graph的过程叫做trace  \n",
    "  \n",
    "retrace  \n",
    "    不同数据signature（shape和dtype定义了signature）的input会导致retrace  \n",
    "    多次传递python args会导致retrace  \n",
    "    @tf.function写在了循环内部  \n",
    "\n",
    "（解释下为什么@tf.function写在了循环内部会导致retrace：  \n",
    "for in loop:  \n",
    "----@tf.function # 修饰  \n",
    "----def usr_def_func: code # 定义  \n",
    "----usr_def_func(input)  # 调用  \n",
    "在循环内部，每一次定义usr_def_func时，都会申请一块新内存，来存储修饰好的usr_def_func。而上一次循环中申请的内存，存储着的usr_def_func，丢了python名字，但因为调用过，所以trace建立了计算图，所以内存还没有释放。  \n",
    "也就是说，在新一次循环，虽然函数名字没变，但它已经指向了新的内存块。此时完成函数调用之后，其实是用新的内存块的函数调用，其实也是第一次调用，于是就会retrace。\n",
    "\n",
    "\n",
    "内存中若同时存在5个tf.Graph，则会报warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "358c856c2c9243453432cede0b7a0ffead5a19b5df3cbac804ae558d1878f503"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
